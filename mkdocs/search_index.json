{
    "docs": [
        {
            "location": "/",
            "text": "",
            "title": "Home"
        },
        {
            "location": "/Advanced/",
            "text": "TOC\n{:toc}\n\n\n\n\n[Note: these features are currently only supported in the main Lua/Torch version of the library. If there is a feature you would like in Python, please vote for it in our \nforum\n.]\n\n\nConfiguration files\n\n\nWhen using the main scripts \npreprocess.lua\n, \ntrain.lua\n and \ntranslate.lua\n, you can pass your options using a configuration file. The file has a simple key-value syntax with one \noption = value\n per line. Here is an example:\n\n\n$ cat generic.txt\nrnn_size = 600\nlayers = 4\nbrnn = true\nsave_model = generic\n\n\n\n\nIt handles empty line and ignore lines prefixed with \n#\n.\n\n\nYou can then pass this file along other options on the command line:\n\n\nth train.lua -config generic.txt -data data/demo-train.t7 -gpuid 1\n\n\n\n\nIf an option appears both in the file and on the command line, the file takes priority.\n\n\nData Preparation\n\n\nWord features\n\n\nOpenNMT supports additional features on source and target words in the form of \ndiscrete labels\n.\n\n\n\n\nOn the source side, these features act as additional information to the encoder. An\nembedding will be optimized for each label and then fed as additional source input\nalongside the word it annotates.\n\n\nOn the target side, these features will be predicted by the network. The\ndecoder is then able to decode a sentence and annotate each word.\n\n\n\n\nTo use additional features, directly modify your data by appending labels to each word with\nthe special character \n\uffe8\n (unicode character FFE8). There can be an arbitrary number of additional\nfeatures in the form \nword\uffe8feat1\uffe8feat2\uffe8...\uffe8featN\n but each word must have the same number of\nfeatures and in the same order. Source and target data can have a different number of additional features.\n\n\nAs an example, see \ndata/src-train-case.txt\n which uses a separate feature\nto represent the case of each word. Using case as a feature is a way to optimize the word\ndictionary (no duplicated words like \"the\" and \"The\") and gives the system an additional\ninformation that can be useful to optimize its objective function.\n\n\nit\uffe8C is\uffe8l not\uffe8l acceptable\uffe8l that\uffe8l ,\uffe8n with\uffe8l the\uffe8l help\uffe8l of\uffe8l the\uffe8l national\uffe8l bureaucracies\uffe8l ,\uffe8n parliament\uffe8C &apos;s\uffe8l legislative\uffe8l prerogative\uffe8l should\uffe8l be\uffe8l made\uffe8l null\uffe8l and\uffe8l void\uffe8l by\uffe8l means\uffe8l of\uffe8l implementing\uffe8l provisions\uffe8l whose\uffe8l content\uffe8l ,\uffe8n purpose\uffe8l and\uffe8l extent\uffe8l are\uffe8l not\uffe8l laid\uffe8l down\uffe8l in\uffe8l advance\uffe8l .\uffe8n\n\n\n\n\nYou can generate this case feature with OpenNMT's tokenization script and the \n-case_feature\n flag.\n\n\nVocabulary\n\n\nBy default, features vocabulary size is unlimited. Depending on the type of features you are using, you may want to limit their vocabulary during the preprocessing with the \n-src_vocab_size\n and \n-tgt_vocab_size\n options in the format \nword_vocab_size[,feat1_vocab_size[,feat2_vocab_size[...]]]\n. For example:\n\n\n# unlimited source features vocabulary size\n-src_vocab_size 50000\n\n# first feature vocabulary is limited to 60, others are unlimited\n-src_vocab_size 50000,60\n\n# second feature vocabulary is limited to 100, others are unlimited\n-src_vocab_size 50000,0,100\n\n# limit vocabulary size of the first and second feature\n-src_vocab_size 50000,60,100\n\n\n\n\nTraining\n\n\nPre-trained embeddings\n\n\nWhen training with small amounts of data, performance can be improved\nby starting with pretrained embeddings. The arguments\n\n-pre_word_vecs_dec\n and \n-pre_word_vecs_enc\n can be used to specify\nthese files. The pretrained embeddings must be manually constructed\ntorch serialized matrices that correspond to the src and tgt\ndictionary files. By default these embeddings will be updated during\ntraining, but they can be held fixed using \n-fix_word_vecs_enc\n and\n\n-fix_word_vecs_dec\n.\n\n\nWord features embeddings\n\n\nThe feature embedding size is automatically computed based on the number of values the feature takes. The default size reduction works well for features with few values like the case or POS. For other features, you may want to manually choose the embedding size with the \n-src_word_vec_size\n and \n-tgt_word_vec_size\n options. They behave similarly to \n-src_vocab_size\n with a comma-separated list of embedding size: \nword_vec_size[,feat1_vec_size[,feat2_vec_size[...]]]\n.\n\n\nBy default each embedding is concatenated. You can choose to sum them by setting \n-feat_merge sum\n. Note that in this case each feature embedding must have the same dimension. You can set the common embedding size with \n-feat_vec_size\n.\n\n\nMulti-GPU training\n\n\nOpenNMT supports \ndata parallelism\n during the training. This technique allows the use of several GPUs by training batches in parallel on different \nnetwork replicas\n. To enable this option, assign a list of comma-separated GPU identifier to the \n-gpuid\n option. For example:\n\n\nth train.lua -data data/demo-train.t7 -save_model demo -gpuid 1,2,4\n\n\n\n\nwill use the first, the second and the fourth GPU of the machine.\n\n\nThere are 2 different modes:\n\n\n\n\nsynchronous parallelism\n (default): in this mode, each replica processes in parallel a different batch at each iteration. The gradients from each replica are accumulated, and parameters updated and synchronized. Note that when using \nN\n GPU(s), the actual batch size is \nN * max_batch_size\n.\n\n\nasynchronous parallelism\n (\n-async_parallel\n flag): in this mode, the different replicas are independently\ncalculating their own gradient, updating a master copy of the parameters and getting updated values\nof the parameters. Note that a GPU core is dedicated to storage of the master copy of the parameters and is not used for training. Also, to enable convergence at the beginning of the training, only one replica is working for the first \n-async_parallel_minbatch\n iterations.\n\n\n\n\nTraining from a saved model\n\n\nBy default, OpenNMT saves a checkpoint at the end of every epoch. For more frequent saves, you can use the \n-save_every\n option which defines the number of iterations after which the training saves a checkpoint.\n\n\nThere are several reasons one may want to train from a saved model with the \n-train_from\n option:\n\n\n\n\ncontinuing a stopped training\n\n\ncontinuing the training with a smaller batch size\n\n\ntraining a model on new data (incremental adaptation)\n\n\nstarting a training from pre-trained parameters\n\n\netc.\n\n\n\n\nResuming a stopped training\n\n\nIt is common that a training stops: crash, server reboot, user action, etc. In this case, you may want to continue the training for more epochs by using using the \n-continue\n option. For example:\n\n\n# start the initial training\nth train.lua -gpuid 1 -data data/demo-train.t7 -save_model demo -save_every 50\n\n# train for several epochs...\n\n# need to reboot the server!\n\n# continue the training from the last checkpoint\nth train.lua -gpuid 1 -data data/demo-train.t7 -save_model demo -save_every 50 -train_from demo_checkpoint.t7 -continue\n\n\n\n\nThe \n-continue\n flag ensures that the training continues with the same configuration and optimization states. In particular, the following options are set to their last known value:\n\n\n\n\ncurriculum\n\n\nlearning_rate_decay\n\n\nlearning_rate\n\n\noptim\n\n\nstart_decay_at\n\n\nstart_epoch\n\n\nstart_iteration\n\n\n\n\nNote:\n the \n-end_epoch\n value is not automatically set as the user may want to continue its training for more epochs past the end.\n\n\nTraining from pre-trained parameters\n\n\nAnother use case it to use a base model and train it further with new training options (in particular the optimization method and the learning rate). Using \n-train_from\n without \n-continue\n will start a new training with parameters initialized from a pre-trained model.\n\n\nNote that the model topology and dropout value can not be changed during a retraining.\n\n\nTranslation\n\n\nTranslation and beam search\n\n\nBy default translation is done using beam search. The \n-beam_size\n\noption can be used to trade-off translation time and search accuracy,\nwith \n-beam_size 1\n giving greedy search. The small default beam size\nis often enough in practice. Beam search can also be used to provide\nan approximate n-best list of translations by setting \n-n_best\n\ngreater than 1. For analysis, the translation command also takes an\noracle/gold \n-tgt\n file and will output a comparison of scores.\n\n\nTranslating unknown words\n\n\nThe default translation mode allows the model to produce the UNK\nsymbol when it is not sure of the specific target word. Often times\nUNK symbols will correspond to proper names that can be directly\ntransposed between languages. The \n-replace_unk\n option will\nsubstitute UNK with a source word using the attention of the\nmodel.\n\n\nAlternatively, advanced users may prefer to provide a\npreconstructed phrase table from an external aligner (such as\nfast_align) using the \n-phrase_table\n option to allow for non-identity replacement.\nInstead of copying the source token with the highest attention, it will\nlookup in the phrase table for a possible translation. If a valid replacement\nis not found then the source token will be copied.\n\n\nThe phrase table is a file with one translation per line in the format:\n\n\nsource|||target\n\n\n\n\nWhere \nsource\n and \ntarget\n are \ncase sensitive\n and \nsingle\n tokens.\n\n\nReleasing models\n\n\nAfter training a model, you may want to release it for inference only by using the \nrelease_model.lua\n script. A released model takes less space on disk and is compatible with CPU translation.\n\n\nth tools/release_model.lua -model model.t7 -gpuid 1\n\n\n\n\nBy default, it will create a \nmodel_release.t7\n file. See \nth tools/release_model.lua -h\n for advanced options.\n\n\nC++ translator\n\n\nOpenNMT also includes an optimized C++-only \ntranslator\n for CPU deployment. The code has no dependencies on Torch or Lua and can be run out of the box with standard OpenNMT models. Simply follow the CPU instructions above to release the model, and then use the \ninstallation instructions\n.\n\n\nThe C++ version takes the same arguments as \ntranslate.lua\n.\n\n\ncli/translate --model model_release.t7 --src src-val.txt\n\n\n\n\nTranslation REST server\n\n\nOpenNMT includes a REST translation server for running translate remotely.\nYou can use an easy REST syntax to simply send plain text.\nSentence will be tokenized, translated and then detokenized.\n\n\nThe server uses the restserver-xavante dependancy, you need to install it by running:\n\n\nluarocks install restserver-xavante\n\n\n\n\nThe translation server can be run using any of the arguments from \ntokenize.lua\n or \ntranslate.lua\n. \n\n\nth tools/rest_translation_server.lua -model ../Recipes/baseline-1M-enfr/exp/model-baseline-1M-enfr_epoch13_3.44.t7 -gpuid 1 -host ... -port -case_feature -bpe_model ...\n\n\n\n\nNote:\n the default host is set to \n127.0.0.1\n , default port is set to \n7784\n.\n\n\nYou can test it with a curl command locally or from any other client.\n\n\ncurl -v -H \"Content-Type: application/json\" -X POST -d '{ \"src\" : \"Hello World }' http://IP_address:7784/translator/translate\n\n\n\n\nAnswer will be embeeded in a JSON format, translated sentence in the \"tgt\" section.\n\n\nAdditionnally you can get the attention matrix with the \n-withAttn\n option in the server command line. \n\n\nTranslation ZMQ server\n\n\nOpenNMT includes a translation server for running translate remotely. This also is an\neasy way to use models from other languages such as Java and Python. \n\n\nThe server uses the 0MQ for RPC. You can install 0MQ and the Lua bindings on Ubuntu by running:\n\n\nsudo apt-get install libzmq-dev\nluarocks install dkjson\nluarocks install lua-zmq ZEROMQ_LIBDIR=/usr/lib/x86_64-linux-gnu/ ZEROMQ_INCDIR=/usr/include\n\n\n\n\nThe translation server can be run using any of the arguments from \ntranslate.lua\n. \n\n\nth tools/translation_server.lua -host ... -port ... -model ...\n\n\n\n\nNote:\n the default host is set to \n127.0.0.1\n which only allows local access. If you want to support remote access, use \n0.0.0.0\n instead.\n\n\nIt runs as a message queue that takes in a JSON batch of src sentences. For example the following 5 lines of Python\ncode can be used to send a single sentence for translation.\n\n\nimport zmq, sys, json\nsock = zmq.Context().socket(zmq.REQ)\nsock.connect(\"tcp://127.0.0.1:5556\")\nsock.send(json.dumps([{\"src\": \" \".join(sys.argv[1:])}]))\nprint sock.recv()\n\n\n\n\nFor a longer example, see our \nPython/Flask server\n in development. \n\n\nExtending the system (Image-to-Text)\n\n\nOpenNMT is explicitly separated out into a library and application \nsection. All modeling and training code can be directly used within\nother Torch applications. \n\n\nAs an example use case we have released an extension for translating\nfrom images-to-text. This model replaces the source-side word\nembeddings with a convolutional image network. The full model is\navailable at \nOpenNMT/im2text\n.",
            "title": "Advanced"
        },
        {
            "location": "/Advanced/#configuration-files",
            "text": "When using the main scripts  preprocess.lua ,  train.lua  and  translate.lua , you can pass your options using a configuration file. The file has a simple key-value syntax with one  option = value  per line. Here is an example:  $ cat generic.txt\nrnn_size = 600\nlayers = 4\nbrnn = true\nsave_model = generic  It handles empty line and ignore lines prefixed with  # .  You can then pass this file along other options on the command line:  th train.lua -config generic.txt -data data/demo-train.t7 -gpuid 1  If an option appears both in the file and on the command line, the file takes priority.",
            "title": "Configuration files"
        },
        {
            "location": "/Advanced/#data-preparation",
            "text": "",
            "title": "Data Preparation"
        },
        {
            "location": "/Advanced/#word-features",
            "text": "OpenNMT supports additional features on source and target words in the form of  discrete labels .   On the source side, these features act as additional information to the encoder. An\nembedding will be optimized for each label and then fed as additional source input\nalongside the word it annotates.  On the target side, these features will be predicted by the network. The\ndecoder is then able to decode a sentence and annotate each word.   To use additional features, directly modify your data by appending labels to each word with\nthe special character  \uffe8  (unicode character FFE8). There can be an arbitrary number of additional\nfeatures in the form  word\uffe8feat1\uffe8feat2\uffe8...\uffe8featN  but each word must have the same number of\nfeatures and in the same order. Source and target data can have a different number of additional features.  As an example, see  data/src-train-case.txt  which uses a separate feature\nto represent the case of each word. Using case as a feature is a way to optimize the word\ndictionary (no duplicated words like \"the\" and \"The\") and gives the system an additional\ninformation that can be useful to optimize its objective function.  it\uffe8C is\uffe8l not\uffe8l acceptable\uffe8l that\uffe8l ,\uffe8n with\uffe8l the\uffe8l help\uffe8l of\uffe8l the\uffe8l national\uffe8l bureaucracies\uffe8l ,\uffe8n parliament\uffe8C &apos;s\uffe8l legislative\uffe8l prerogative\uffe8l should\uffe8l be\uffe8l made\uffe8l null\uffe8l and\uffe8l void\uffe8l by\uffe8l means\uffe8l of\uffe8l implementing\uffe8l provisions\uffe8l whose\uffe8l content\uffe8l ,\uffe8n purpose\uffe8l and\uffe8l extent\uffe8l are\uffe8l not\uffe8l laid\uffe8l down\uffe8l in\uffe8l advance\uffe8l .\uffe8n  You can generate this case feature with OpenNMT's tokenization script and the  -case_feature  flag.",
            "title": "Word features"
        },
        {
            "location": "/Advanced/#vocabulary",
            "text": "By default, features vocabulary size is unlimited. Depending on the type of features you are using, you may want to limit their vocabulary during the preprocessing with the  -src_vocab_size  and  -tgt_vocab_size  options in the format  word_vocab_size[,feat1_vocab_size[,feat2_vocab_size[...]]] . For example:  # unlimited source features vocabulary size\n-src_vocab_size 50000\n\n# first feature vocabulary is limited to 60, others are unlimited\n-src_vocab_size 50000,60\n\n# second feature vocabulary is limited to 100, others are unlimited\n-src_vocab_size 50000,0,100\n\n# limit vocabulary size of the first and second feature\n-src_vocab_size 50000,60,100",
            "title": "Vocabulary"
        },
        {
            "location": "/Advanced/#training",
            "text": "",
            "title": "Training"
        },
        {
            "location": "/Advanced/#pre-trained-embeddings",
            "text": "When training with small amounts of data, performance can be improved\nby starting with pretrained embeddings. The arguments -pre_word_vecs_dec  and  -pre_word_vecs_enc  can be used to specify\nthese files. The pretrained embeddings must be manually constructed\ntorch serialized matrices that correspond to the src and tgt\ndictionary files. By default these embeddings will be updated during\ntraining, but they can be held fixed using  -fix_word_vecs_enc  and -fix_word_vecs_dec .",
            "title": "Pre-trained embeddings"
        },
        {
            "location": "/Advanced/#word-features-embeddings",
            "text": "The feature embedding size is automatically computed based on the number of values the feature takes. The default size reduction works well for features with few values like the case or POS. For other features, you may want to manually choose the embedding size with the  -src_word_vec_size  and  -tgt_word_vec_size  options. They behave similarly to  -src_vocab_size  with a comma-separated list of embedding size:  word_vec_size[,feat1_vec_size[,feat2_vec_size[...]]] .  By default each embedding is concatenated. You can choose to sum them by setting  -feat_merge sum . Note that in this case each feature embedding must have the same dimension. You can set the common embedding size with  -feat_vec_size .",
            "title": "Word features embeddings"
        },
        {
            "location": "/Advanced/#multi-gpu-training",
            "text": "OpenNMT supports  data parallelism  during the training. This technique allows the use of several GPUs by training batches in parallel on different  network replicas . To enable this option, assign a list of comma-separated GPU identifier to the  -gpuid  option. For example:  th train.lua -data data/demo-train.t7 -save_model demo -gpuid 1,2,4  will use the first, the second and the fourth GPU of the machine.  There are 2 different modes:   synchronous parallelism  (default): in this mode, each replica processes in parallel a different batch at each iteration. The gradients from each replica are accumulated, and parameters updated and synchronized. Note that when using  N  GPU(s), the actual batch size is  N * max_batch_size .  asynchronous parallelism  ( -async_parallel  flag): in this mode, the different replicas are independently\ncalculating their own gradient, updating a master copy of the parameters and getting updated values\nof the parameters. Note that a GPU core is dedicated to storage of the master copy of the parameters and is not used for training. Also, to enable convergence at the beginning of the training, only one replica is working for the first  -async_parallel_minbatch  iterations.",
            "title": "Multi-GPU training"
        },
        {
            "location": "/Advanced/#training-from-a-saved-model",
            "text": "By default, OpenNMT saves a checkpoint at the end of every epoch. For more frequent saves, you can use the  -save_every  option which defines the number of iterations after which the training saves a checkpoint.  There are several reasons one may want to train from a saved model with the  -train_from  option:   continuing a stopped training  continuing the training with a smaller batch size  training a model on new data (incremental adaptation)  starting a training from pre-trained parameters  etc.",
            "title": "Training from a saved model"
        },
        {
            "location": "/Advanced/#resuming-a-stopped-training",
            "text": "It is common that a training stops: crash, server reboot, user action, etc. In this case, you may want to continue the training for more epochs by using using the  -continue  option. For example:  # start the initial training\nth train.lua -gpuid 1 -data data/demo-train.t7 -save_model demo -save_every 50\n\n# train for several epochs...\n\n# need to reboot the server!\n\n# continue the training from the last checkpoint\nth train.lua -gpuid 1 -data data/demo-train.t7 -save_model demo -save_every 50 -train_from demo_checkpoint.t7 -continue  The  -continue  flag ensures that the training continues with the same configuration and optimization states. In particular, the following options are set to their last known value:   curriculum  learning_rate_decay  learning_rate  optim  start_decay_at  start_epoch  start_iteration   Note:  the  -end_epoch  value is not automatically set as the user may want to continue its training for more epochs past the end.",
            "title": "Resuming a stopped training"
        },
        {
            "location": "/Advanced/#training-from-pre-trained-parameters",
            "text": "Another use case it to use a base model and train it further with new training options (in particular the optimization method and the learning rate). Using  -train_from  without  -continue  will start a new training with parameters initialized from a pre-trained model.  Note that the model topology and dropout value can not be changed during a retraining.",
            "title": "Training from pre-trained parameters"
        },
        {
            "location": "/Advanced/#translation",
            "text": "",
            "title": "Translation"
        },
        {
            "location": "/Advanced/#translation-and-beam-search",
            "text": "By default translation is done using beam search. The  -beam_size \noption can be used to trade-off translation time and search accuracy,\nwith  -beam_size 1  giving greedy search. The small default beam size\nis often enough in practice. Beam search can also be used to provide\nan approximate n-best list of translations by setting  -n_best \ngreater than 1. For analysis, the translation command also takes an\noracle/gold  -tgt  file and will output a comparison of scores.",
            "title": "Translation and beam search"
        },
        {
            "location": "/Advanced/#translating-unknown-words",
            "text": "The default translation mode allows the model to produce the UNK\nsymbol when it is not sure of the specific target word. Often times\nUNK symbols will correspond to proper names that can be directly\ntransposed between languages. The  -replace_unk  option will\nsubstitute UNK with a source word using the attention of the\nmodel.  Alternatively, advanced users may prefer to provide a\npreconstructed phrase table from an external aligner (such as\nfast_align) using the  -phrase_table  option to allow for non-identity replacement.\nInstead of copying the source token with the highest attention, it will\nlookup in the phrase table for a possible translation. If a valid replacement\nis not found then the source token will be copied.  The phrase table is a file with one translation per line in the format:  source|||target  Where  source  and  target  are  case sensitive  and  single  tokens.",
            "title": "Translating unknown words"
        },
        {
            "location": "/Advanced/#releasing-models",
            "text": "After training a model, you may want to release it for inference only by using the  release_model.lua  script. A released model takes less space on disk and is compatible with CPU translation.  th tools/release_model.lua -model model.t7 -gpuid 1  By default, it will create a  model_release.t7  file. See  th tools/release_model.lua -h  for advanced options.",
            "title": "Releasing models"
        },
        {
            "location": "/Advanced/#c-translator",
            "text": "OpenNMT also includes an optimized C++-only  translator  for CPU deployment. The code has no dependencies on Torch or Lua and can be run out of the box with standard OpenNMT models. Simply follow the CPU instructions above to release the model, and then use the  installation instructions .  The C++ version takes the same arguments as  translate.lua .  cli/translate --model model_release.t7 --src src-val.txt",
            "title": "C++ translator"
        },
        {
            "location": "/Advanced/#translation-rest-server",
            "text": "OpenNMT includes a REST translation server for running translate remotely.\nYou can use an easy REST syntax to simply send plain text.\nSentence will be tokenized, translated and then detokenized.  The server uses the restserver-xavante dependancy, you need to install it by running:  luarocks install restserver-xavante  The translation server can be run using any of the arguments from  tokenize.lua  or  translate.lua .   th tools/rest_translation_server.lua -model ../Recipes/baseline-1M-enfr/exp/model-baseline-1M-enfr_epoch13_3.44.t7 -gpuid 1 -host ... -port -case_feature -bpe_model ...  Note:  the default host is set to  127.0.0.1  , default port is set to  7784 .  You can test it with a curl command locally or from any other client.  curl -v -H \"Content-Type: application/json\" -X POST -d '{ \"src\" : \"Hello World }' http://IP_address:7784/translator/translate  Answer will be embeeded in a JSON format, translated sentence in the \"tgt\" section.  Additionnally you can get the attention matrix with the  -withAttn  option in the server command line.",
            "title": "Translation REST server"
        },
        {
            "location": "/Advanced/#translation-zmq-server",
            "text": "OpenNMT includes a translation server for running translate remotely. This also is an\neasy way to use models from other languages such as Java and Python.   The server uses the 0MQ for RPC. You can install 0MQ and the Lua bindings on Ubuntu by running:  sudo apt-get install libzmq-dev\nluarocks install dkjson\nluarocks install lua-zmq ZEROMQ_LIBDIR=/usr/lib/x86_64-linux-gnu/ ZEROMQ_INCDIR=/usr/include  The translation server can be run using any of the arguments from  translate.lua .   th tools/translation_server.lua -host ... -port ... -model ...  Note:  the default host is set to  127.0.0.1  which only allows local access. If you want to support remote access, use  0.0.0.0  instead.  It runs as a message queue that takes in a JSON batch of src sentences. For example the following 5 lines of Python\ncode can be used to send a single sentence for translation.  import zmq, sys, json\nsock = zmq.Context().socket(zmq.REQ)\nsock.connect(\"tcp://127.0.0.1:5556\")\nsock.send(json.dumps([{\"src\": \" \".join(sys.argv[1:])}]))\nprint sock.recv()  For a longer example, see our  Python/Flask server  in development.",
            "title": "Translation ZMQ server"
        },
        {
            "location": "/Advanced/#extending-the-system-image-to-text",
            "text": "OpenNMT is explicitly separated out into a library and application \nsection. All modeling and training code can be directly used within\nother Torch applications.   As an example use case we have released an extension for translating\nfrom images-to-text. This model replaces the source-side word\nembeddings with a convolutional image network. The full model is\navailable at  OpenNMT/im2text .",
            "title": "Extending the system (Image-to-Text)"
        },
        {
            "location": "/Guide/",
            "text": "TOC\n{:toc}\n\n\n\n\nInstallation\n\n\nOpenNMT only requires a vanilla \nTorch\n install. It makes use of the libraries: nn, nngraph, cunn and bit32. We highly recommend use of GPU-based training.\n\n\nAlternatively there is a \nDocker image\n available.\n\n\nQuickstart\n\n\nOpenNMT consists of three steps. (These steps assume you data is already tokenized. If not see the tokenization section below.)\n\n\n1) Preprocess the data.\n\n\nth preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\n\n\n2) Train the model.\n\n\nth train.lua -data data/demo-train.t7 -save_model model [-gpuid 1]\n\n\n3) Translate sentences.\n\n\nth translate.lua -model model_final.t7 -src data/src-val.txt -output file-tgt.tok [-gpuid 1]\n\n\nLet's walk through each of these commands in more detail. \n\n\nStep 1: Preprocess Data\n\n\nth preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\n\n\nHere we are working with example data in \ndata/\n folder.\nThe data consists of a source (\nsrc\n) and target (\ntgt\n) data.\nThis will take the source/target train/valid files (\nsrc-train.txt, tgt-train.txt,\nsrc-val.txt, tgt-val.txt\n). \n\n\nGet the full options list on the \nfeatures page\n.\n\n\n\n\ndata/tgt-train.txt\n\n\n\n\nEs geht nicht an , dass \u00fcber Ausf\u00fchrungsbestimmungen , deren Inhalt , Zweck und Ausma\u00df vorher nicht bestimmt ist , zusammen mit den nationalen B\u00fcrokratien das Gesetzgebungsrecht des Europ\u00e4ischen Parlaments ausgehebelt wird .\nMeistertrainer und leitender Dozent des italienischen Fitnessverbands f\u00fcr Aerobic , Gruppenfitness , Haltungsgymnastik , Stretching und Pilates; arbeitet seit 2004 bei Antiche Terme als Personal Trainer und Lehrer f\u00fcr Stretching , Pilates und R\u00fcckengymnastik .\nAlso kam ich nach S\u00fcdafrika \" , erz\u00e4hlte eine Frau namens Grace dem Human Rights Watch-Mitarbeiter Gerry Simpson , der die Probleme der zimbabwischen Fl\u00fcchtlinge in S\u00fcdafrika untersucht .\n\n\n\n\n\n\ndata/src-train.txt\n\n\n\n\nIt is not acceptable that , with the help of the national bureaucracies , Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .\n&quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\n\n\n\n\nAfter running the system will build the following files:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.tgt.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized Torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check vocabulary, or to preprocess data with fixed vocabularies.\nThese files are simple human-readable dictionaries.\n\n\n\n\ndata/demo.src.dict\n\n\n\n\n<blank> 1\n<unk> 2\n<s> 3\n</s> 4\nIt 5\nis 6\nnot 7\nacceptable 8\nthat 9\n, 10\nwith 11\n\n\n\n\nInternally the system never touches the words themselves, but uses these indices.\n\n\nStep 2: Train the model\n\n\nth train.lua -data data/demo-train.t7 -save_model demo-model\n\n\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add \n-gpuid 1\n to use (say) GPU 1.\n\n\nGet the full options list on the \nfeatures page\n.\n\n\nStep 3: Translate\n\n\nth translate.lua -model demo-model_final.t7 -src data/src-val.txt -output file-tgt.tok [-gpuid 1]\n\n\nNow you have a model which you can use to predict on new data. We do this by running beam search.\n\n\nThis will output predictions into \nfile-tgt.tok\n. The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for \ntranslation\n\nor \nsummarization\n.\n\n\nGet the full options list on the \nfeatures page\n.\n\n\nTokenization\n\n\nOur demo data comes pre-tokenized. If you data is not tokenized, we also provide a simple language-independent tokenizer/detokenizer that we have found to be effective for machine translation tasks. You can run the tokenizer with the following commands:\n\n\n1) Tokenize your data (file is src-train.txt, tgt-train.txt, valid and test sets as well)\n\n\nth tools/tokenize.lua -joiner_annotate < file > file.tok\n\n\nThe \ntokenize.lua\n script can also handle following options:\n\n\n\n\n-mode\n: can be \naggressive\n or \nconservative\n (default). In conservative mode, letters, numbers and '_' are kept in sequence, hyphens are accepted as part of tokens. Finally inner characters \n[.,]\n are also accepted (url, numbers).\n\n\n-case_feature\n: generate case feature - and convert all tokens to lowercase\n\n\n\n\nGet the full options list on the \nfeatures page\n.\n\n\n2) Detokenize the output.\n\n\nth tools/detokenize.lua < file-tgt.tok > file-tgt.detok\n\n\nThe \ndetokenize.lua\n script can also handle following options:\n\n\n\n\n-case_feature\n: indicating that the first feature is a case feature - restore the cases on the tokens",
            "title": "Guide"
        },
        {
            "location": "/Guide/#installation",
            "text": "OpenNMT only requires a vanilla  Torch  install. It makes use of the libraries: nn, nngraph, cunn and bit32. We highly recommend use of GPU-based training.  Alternatively there is a  Docker image  available.",
            "title": "Installation"
        },
        {
            "location": "/Guide/#quickstart",
            "text": "OpenNMT consists of three steps. (These steps assume you data is already tokenized. If not see the tokenization section below.)  1) Preprocess the data.  th preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo  2) Train the model.  th train.lua -data data/demo-train.t7 -save_model model [-gpuid 1]  3) Translate sentences.  th translate.lua -model model_final.t7 -src data/src-val.txt -output file-tgt.tok [-gpuid 1]  Let's walk through each of these commands in more detail.",
            "title": "Quickstart"
        },
        {
            "location": "/Guide/#step-1-preprocess-data",
            "text": "th preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo  Here we are working with example data in  data/  folder.\nThe data consists of a source ( src ) and target ( tgt ) data.\nThis will take the source/target train/valid files ( src-train.txt, tgt-train.txt,\nsrc-val.txt, tgt-val.txt ).   Get the full options list on the  features page .   data/tgt-train.txt   Es geht nicht an , dass \u00fcber Ausf\u00fchrungsbestimmungen , deren Inhalt , Zweck und Ausma\u00df vorher nicht bestimmt ist , zusammen mit den nationalen B\u00fcrokratien das Gesetzgebungsrecht des Europ\u00e4ischen Parlaments ausgehebelt wird .\nMeistertrainer und leitender Dozent des italienischen Fitnessverbands f\u00fcr Aerobic , Gruppenfitness , Haltungsgymnastik , Stretching und Pilates; arbeitet seit 2004 bei Antiche Terme als Personal Trainer und Lehrer f\u00fcr Stretching , Pilates und R\u00fcckengymnastik .\nAlso kam ich nach S\u00fcdafrika \" , erz\u00e4hlte eine Frau namens Grace dem Human Rights Watch-Mitarbeiter Gerry Simpson , der die Probleme der zimbabwischen Fl\u00fcchtlinge in S\u00fcdafrika untersucht .   data/src-train.txt   It is not acceptable that , with the help of the national bureaucracies , Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .\n&quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .  After running the system will build the following files:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.tgt.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized Torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check vocabulary, or to preprocess data with fixed vocabularies.\nThese files are simple human-readable dictionaries.   data/demo.src.dict   <blank> 1\n<unk> 2\n<s> 3\n</s> 4\nIt 5\nis 6\nnot 7\nacceptable 8\nthat 9\n, 10\nwith 11  Internally the system never touches the words themselves, but uses these indices.",
            "title": "Step 1: Preprocess Data"
        },
        {
            "location": "/Guide/#step-2-train-the-model",
            "text": "th train.lua -data data/demo-train.t7 -save_model demo-model  The main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add  -gpuid 1  to use (say) GPU 1.  Get the full options list on the  features page .",
            "title": "Step 2: Train the model"
        },
        {
            "location": "/Guide/#step-3-translate",
            "text": "th translate.lua -model demo-model_final.t7 -src data/src-val.txt -output file-tgt.tok [-gpuid 1]  Now you have a model which you can use to predict on new data. We do this by running beam search.  This will output predictions into  file-tgt.tok . The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for  translation \nor  summarization .  Get the full options list on the  features page .",
            "title": "Step 3: Translate"
        },
        {
            "location": "/Guide/#tokenization",
            "text": "Our demo data comes pre-tokenized. If you data is not tokenized, we also provide a simple language-independent tokenizer/detokenizer that we have found to be effective for machine translation tasks. You can run the tokenizer with the following commands:  1) Tokenize your data (file is src-train.txt, tgt-train.txt, valid and test sets as well)  th tools/tokenize.lua -joiner_annotate < file > file.tok  The  tokenize.lua  script can also handle following options:   -mode : can be  aggressive  or  conservative  (default). In conservative mode, letters, numbers and '_' are kept in sequence, hyphens are accepted as part of tokens. Finally inner characters  [.,]  are also accepted (url, numbers).  -case_feature : generate case feature - and convert all tokens to lowercase   Get the full options list on the  features page .  2) Detokenize the output.  th tools/detokenize.lua < file-tgt.tok > file-tgt.detok  The  detokenize.lua  script can also handle following options:   -case_feature : indicating that the first feature is a case feature - restore the cases on the tokens",
            "title": "Tokenization"
        }
    ]
}